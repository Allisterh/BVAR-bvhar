---
title: "Sparse Priors"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Sparse Priors}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  \newcommand{\R}{\mathbb{R}}
  \newcommand{\bbA}{\mathbb{A}}
  \newcommand{\bfeps}{\boldsymbol\epsilon}
  \newcommand{\iid}{\stackrel{iid}{\sim}}
  \newcommand{\bfy}{\mathbf{y}}
  \newcommand{\bfc}{\mathbf{c}}
---

```{r rmdsetup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "70%",
  fig.align = "center",
  fig.width = 6,
  fig.asp = .618
)
options(digits = 3)
set.seed(1)
```

```{r setup}
library(bvhar)
```

```{r otherpkg, message=FALSE}
# Bayes plot---------
library(bayesplot)
```


# Stochastic Search Variable Selection (SSVS)

## Spike-and-Slab

## SSVS

### Default semi-automatic approach

### Covariance shrinkage

$$\Sigma^{-1} = \Psi \Psi^\intercal$$

where $\Psi$ is an upper triangular matrix. Shrinking $\Psi$ gives the parsimony of precision matrix $\Sigma^{-1}$.

## Example 1 of George et al. (2008)

### VAR simulation

6 variable VAR(1) with VAR coefficient $\bbA$:

```{r truecoef}
var_coef <- matrix(0L, nrow = 7, ncol = 6)
diag(var_coef[-7,]) <- 1
# diag(var_coef[-7,]) <- .9
var_coef[7,] <- 1
var_coef
```

and Choleksy factor $\Psi$:

```{r varsetting}
# Psi-------------------------------------
var_chol <- diag(6)
var_chol[1,2:6] <- .5
var_chol
```

Note that the variance matrix for simulation is $\Sigma_e = (\Psi \Psi^\intercal)^{-1}$.

- MC simulation: 100 Samples of Size $T = 50$.
    - In this vignette, we only generate 1 sample, not MC simulation.
    - This is because our goal is to show how the functions work, not reproduce the paper exactly.
- Following Appendix D.1 of Helmut (2008), we compute the square root of multivariate normal distribution variance matrix by Cholesky decomposition.

```{r varsimul}
var_data <- 
  sim_var(
    num_sim = 100,
    num_burn = 50,
    var_coef = var_coef,
    var_lag = 1,
    sig_error = solve(var_chol %*% t(var_chol)),
    init = matrix(runif(ncol(var_coef)), nrow = 1, ncol = ncol(var_coef)),
    method = "chol"
  )
colnames(var_data) <- paste0("y", 1:6)
```

### SSVS spec

`set_ssvs()` specifies SSVS input.

- `coef_spike`: $\tau_{0i} = 0.1$
- `coef_slab`: $\tau_{1i} = 5$
    - For semi-automatic approach to $(\tau_{0i}, \tau_{1i})$, see the related work such as George et al. (2008)
- `coef_mixture`: $p_j = 0.5$
    - noninformative
    - equally likely to be included as excluded
- `shape`: $a_j = 0.01$
- `rate`: $b_j = 0.01$
    - absence of prior information about $\psi_{jj}$
    - make the prior noninfluential with the hyperparameters $(a_j, b_j)$ set to small values
- `chol_spike`: $\kappa_{0ij} = 0.1$
- `chol_slab`: $\kappa_{1ij} = 5$
    - For semi-automatic approach to $(\kappa_{0ij}, \kappa_{1ij})$, see the related work such as George et al. (2008)
- `chol_mixture`: $q_{ij} = 0.5$
    - noninformative
    - equally likely to be included as excluded

As every hyperparameter has set to be the same for every $i,j$, value can be assigned as numeric value of length one.
If you want to assign individually, you can use vector.
On the other hand, you can use upper triangular matrix for `chol_spike`, `chol_slab`, and `chol_mixture`.

```{r paramset}
(ssvs_spec <- set_ssvs(
  coef_spike = .1,
  coef_slab = 5,
  coef_mixture = .5,
  coef_non = 5,
  shape = .01,
  rate = .01,
  chol_spike = .1,
  chol_slab = 5,
  chol_mixture = .5
))
```

Before starting MCMC, set initial values.
`init_ssvs()` sets initial values for each parameter.
Since George et al. (2008) did not mention how they specify the starting points, we initialize as follows.
If the initial values are set close to posterior modes, MCMC is known to be converged fast.
Since Geoge et al. (2008) has already provides the average of posterior mean over 100 sample, we use them as initial points.

- `init_coef`: initial $\bbA = 0_{(kp + 1) \times k}$, k = 6, p = 1
- `init_coef_dummy`: initial $\Gamma = 1_{kp \times k}$, k = 6, p = 1
- `init_chol`: initial $\Psi = 0_{k \times \times}$ k = 6
- `init_chol_dummy`: initial $\Omega =$ upper triangular matrix of which every diagonal and upper diagonal element is 1.

```{r initset}
# average posterior means of parameters-------
init_coef <- matrix(c(
  .87, .06, .07, .06, .07, .06,
  .03, .80, .03, .04, .02, .04,
  .02, .04, .81, .04, .04, .03,
  .02, .03, .03, .80, .03, .04,
  .03, .03, .03, .03, .81, .03,
  .02, .04, .03, .03, .03, .79,
  1.11, .95, .93, 1.12, 1.05, .98
), nrow = 7, byrow = TRUE)
init_coef_dummy <- matrix(c(
  1.00, .04, rep(.05, 4),
  .04, 1.00, rep(.04, 4),
  .04, .05, 1.00, .05, .04, .04,
  rep(.04, 3), 1.00, .04, .05,
  rep(.04, 4), 1.00, .04,
  rep(.04, 5), 1.00
), nrow = 6, byrow = TRUE)
init_chol <- matrix(0L, nrow = 6L, ncol = 6L)
diag(init_chol) <- c(1.09, 1.08, 1.08, 1.11, 1.10, 1.10)
init_chol[upper.tri(init_chol, diag = FALSE)] <- c(
  .42, 
  .40, -.04, 
  .41, rep(-.03, 2), 
  .39, rep(-.03, 3),
  .38, -.04, -.03, rep(-.04, 2)
)
init_omega <- matrix(0L, nrow = 6L, ncol = 6L)
diag(init_omega) <- 1
init_omega[upper.tri(init_omega, diag = FALSE)] <- c(
  .58,
  .55, .08,
  .53, .08, .07,
  .52, .07, rep(.08, 2),
  .50, rep(.08, 2), .10, .09
)
# initial values specification-----------------
(init_spec <- init_ssvs(
  init_coef = init_coef,
  init_coef_dummy = init_coef_dummy,
  init_chol = init_chol,
  init_chol_dummy = init_omega
))
```

### Gibbs sampling

`bvar_ssvs()` performs SSVS for VAR model.

- `y`: data
- `p`: order
- `num_iter`: Total number of iteration (including burn-in)
- `num_burn`: The number of burn-in (warm-up)
- `thinning`: Thinning
- `bayes_spec`: SSVS specification using `set_ssvs()`.
- `init_spec`: SSVS initialization specification using `init_ssvs()`.
- `include_mean`: whether including the constant term in the model.
- `verbose`: whether to display the progress bar.

```{r gibbs}
(var_ssvs <- bvar_ssvs(
  y = var_data,
  p = 1,
  num_iter = 1000,
  num_burn = 500,
  thinning = 3,
  bayes_spec = ssvs_spec,
  init_spec = init_spec,
  include_mean = TRUE,
  verbose = FALSE
))
```


## Plots for MCMC

Since each parameter element is `posterior::draws_df` format, `bayesplot` package is applicable.

```{r psiplot}
color_scheme_set("blue")
autoplot(var_ssvs, type = "trace", regex_pars = "psi")
```

```{r alphadens}
autoplot(var_ssvs, type = "dens", pars = c("alpha[1]", "alpha[2]", "alpha[9]"))
```


## SSVS for VHAR Model

6 variable VHAR:

```{r initvhar}
# Upper triangular matrix----------------------
initvhar_chol <- matrix(0L, nrow = 6L, ncol = 6L)
diag(initvhar_chol) <- .1
initvhar_omega <- matrix(1L, nrow = 6L, ncol = 6L)
initvhar_omega[lower.tri(initvhar_omega, diag = TRUE)] <- 0L
# initial values specification-----------------
(initvhar_spec <- init_ssvs(
  init_coef = matrix(0L, nrow = 19L, ncol = 6L),
  init_coef_dummy = matrix(1L, nrow = 18L, ncol = 6L),
  init_chol = initvhar_chol,
  init_chol_dummy = initvhar_omega
))
```

```{r gibbsvhar}
(vhar_ssvs <- bvhar_ssvs(
  y = var_data,
  har = c(5, 22),
  num_iter = 1000,
  num_burn = 500,
  thinning = 1,
  bayes_spec = ssvs_spec,
  init_spec = initvhar_spec,
  include_mean = TRUE,
  verbose = FALSE
))
```


# Horseshoe Prior

## Horseshoe Prior for VAR

### Horseshoe spec

- `init_local`: local shrinkage level $\lambda_j$
- `init_global`: global shrinkage level $\tau$
- `init_priorvar`: covariance matrix of the data

```{r hsspec}
(hs_spec <- set_horseshoe(local_sparsity = rep(.2, 7), global_sparsity = .1, cov(var_data)))
```

### Gibbs sampling

`bvar_horseshoe()` performs BVAR model with horseshoe prior estimation.

- `y`: data
- `p`: order
- `num_iter`: Total number of iteration (including burn-in)
- `num_burn`: The number of burn-in (warm-up)
- `thinning`: Thinning
- `bayes_spec`: Horseshoe specification using `set_horseshoe()`.
- `include_mean`: whether including the constant term in the model.
- `verbose`: whether to display the progress bar.

```{r hsgibbs}
(var_hs <- bvar_horseshoe(
  y = var_data,
  p = 1,
  num_iter = 1000,
  num_burn = 500,
  thinning = 1,
  bayes_spec = hs_spec,
  include_mean = TRUE,
  fast_sampling = FALSE,
  verbose = FALSE
))
```

## Horseshoe Prior for VHAR

Local shrinkage levels are also given to each row of the coefficients matrix.
Here, VHAR coefficient.

```{r hsvharspec}
(hsvhar_spec <- set_horseshoe(local_sparsity = rep(.2, 3 * ncol(var_data) + 1), global_sparsity = .1, cov(var_data)))
```

`bvhar_horseshoe()` performs BVHAR model with horseshoe prior estimation.

- `y`: data
- `har`: `(week, month)` order.
- `num_iter`: Total number of iteration (including burn-in)
- `num_burn`: The number of burn-in (warm-up)
- `thinning`: Thinning
- `bayes_spec`: Horseshoe specification using `set_horseshoe()`.
- `include_mean`: whether including the constant term in the model.
- `verbose`: whether to display the progress bar.

```{r hsvharfit}
(vhar_hs <- bvhar_horseshoe(
  y = var_data,
  har = c(5, 22),
  num_iter = 1000,
  num_burn = 500,
  thinning = 1,
  bayes_spec = hsvhar_spec,
  include_mean = TRUE,
  fast_sampling = FALSE,
  verbose = FALSE
))
```


