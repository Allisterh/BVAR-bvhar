---
title: "Introduction to BVAR"
output: rmarkdown::html_vignette
header-includes:
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\B}{\boldsymbol\beta}
- \newcommand{\hb}{\boldsymbol{\hat\beta}}
- \newcommand{\E}{\boldsymbol\epsilon}
- \DeclareMathOperator*{\argmin}{argmin}
- \DeclareMathOperator*{\argmax}{argmax}
- \newcommand{\defn}{\mathpunct{:}=}
- \newcommand{\X}{\mathbf{X}}
- \newcommand{\Y}{\mathbf{Y}}
- \newcommand{\by}{\mathbf{y}}
- \newcommand{\bz}{\mathbf{Z}}
- \newcommand{\ba}{\boldsymbol{\alpha}}
- \newcommand{\bc}{\mathbf{c}}
- \newcommand{\bu}{\mathbf{u}}
- \def\Cov{\mathrm{Cov}}
- \def\Var{\mathrm{Var}}
- \def\Corr{\mathrm{Corr}}
- \def\vec{\mathrm{vec}}
vignette: >
  %\VignetteIndexEntry{Introduction to BVAR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  out.width = "70%",
  fig.align = "center",
  fig.width = 6,
  fig.asp = .618,
  fig.pos = "H"
  )
options(digits = 3)
```

```{r pkgs}
library(bvhar)
```

```{r addpkgs, message=FALSE}
# for this vignette--------------
library(dplyr)
```

# Simple Setting

## ETF Dataset

Among the datasets in this package, we try CBOE ETF volatility index (`etf_vix`).
Aribitrarily, we extract a small number of variables: *Gold, crude oil, euro currency, and china ETF*.

```{r etfdat}
var_idx <- c("GVZCLS", "OVXCLS", "EVZCLS", "VXFXICLS")
etf <- 
  etf_vix %>% 
  select(all_of(var_idx))
etf
```

For out-of-sample forecasting, split the data.
The last `19` observations will be test set.

```{r outofsample}
h <- 19
# train-------------
etf_train <- 
  etf %>% 
  slice(seq_len(n() - h))
# test--------------
etf_test <- setdiff(etf, etf_train)
# dimension---------
m <- ncol(etf)
```

- n: Total number of observation
- p: VAR lag
- m: Dimension of variable
- s = n - p
- k = m * p + 1

# Minnesota Prior

- Litterman (1986) and Bańbura et al. (2010)
- All the equations are centered around the random walk with drift.
- *Prior mean*: Recent lags provide more reliable information than the more distant ones.
- *Prior variance*: Own lags explain more of the variation of a given variable than the lags of other variables in the equation.

## Fit

```{r bvarlag}
bvar_lag <- 5
```

Set Minnesota parameters.

- `sigma`: Vector $\sigma_1, \ldots, \sigma_m$
    - $\Sigma_e = diag(\sigma_1^2, \ldots, \sigma_m^2)$
    - $\sigma_i^2 / \sigma_j^2$: different scale and variability of the data
- `lambda`
    - Controls the overall tightness of the prior distribution around the RW or WN
    - Governs the relative importance of the prior beliefs w.r.t. the information contained in the data
        - If $\lambda = 0$, then posterior = prior and the data do not influence the estimates.
        - If $\lambda = \infty$, then posterior expectations = OLS.
    - Choose in relation to the size of the system (Bańbura et al. (2010))
        - As `m` increases, $\lambda$ should be smaller to avoid overfitting (De Mol et al. (2008))
- `delta`: Persistence
    - Litterman (1986) originally sets high persistence $\delta_i = 1$
    - For Non-stationary variables: random walk prior $\delta_i = 1$
    - For stationary variables: white noise prior $\delta_i = 0$
- `eps`: Very small number to make matrix invertible

```{r minnesotaset}
sig <- apply(etf_train, 2, sd) # sigma vector
lam <- .2 # lambda
delta <- rep(0, m) # delta vector (0 vector since RV stationary)
eps <- 1e-04 # very small number
```

```{r bvarfit}
(fit_bvar <- bvar_minnesota(etf_train, bvar_lag, sig, lam, delta, eps))
```

We provide residual plot with `autoplot` method:

```{r}
autoplot(fit_bvar, shape = 1, alpha = .5)
```

## Posterior Distribution

```{r}
est_bvar <- gen_posterior(fit_bvar, 500)
```

This generates Normal-IW.

```{r}
class(est_bvar)
names(est_bvar)
```

Also, you can draw density plot for coefficients using `autoplot`.
For example, corresponding to `GVZCLS`:

```{r}
autoplot(est_bvar, "GVZCLS")
```

## Forecasting

```{r}
(pred_bvar <- predict(fit_bvar, h))
```

```{r}
(mse_bvar <- apply(etf_test - pred_bvar, 2, function(x) mean(x^2)))
```

Compare:

```{r}
# VAR-------------------------
fit_var <- var_lm(etf_train, bvar_lag)
pred_var <- predict(fit_var, h)
mse_var <- apply(etf_test - pred_var, 2, function(x) mean(x^2))
# VHAR------------------------
fit_har <- vhar_lm(etf_train)
pred_har <- predict(fit_har, h)
mse_har <- apply(etf_test - pred_har, 2, function(x) mean(x^2))
# BVHAR-----------------------
fit_bvhar <- bvhar_minnesota(etf_train, sig, lam, delta, eps)
pred_bvhar <- predict(fit_bvhar, h)
mse_bvhar <- apply(etf_test - pred_bvhar, 2, function(x) mean(x^2))
```

MSE:

```{r}
# VAR-------------
mean(mse_var)
# VHAR------------
mean(mse_har)
# BVAR------------
mean(mse_bvar)
# BVHAR-----------
mean(mse_bvhar)
```

# Ghosh Prior

## Fit

If you set $U = cI_k$, posterior mean is equivalent to ridge estimator.
Ghosh et al. (2018) had chosen a value from `[0, 10]` using deviance information criteria (DIC).
Its selection might be the user's decision.

```{r}
U <- 5000 * diag(m * bvar_lag + 1) # c * I
(fit_ghosh <- bvar_ghosh(etf_train, bvar_lag, U))
```

## Forecasting

```{r}
(pred_ghosh <- predict(fit_ghosh, h))
```

```{r}
(mse_ghosh <- apply(etf_test - pred_ghosh, 2, function(x) mean(x^2)))
```

```{r}
# VAR-------------
mean(mse_var)
# VHAR------------
mean(mse_har)
# Minnesota-------
mean(mse_bvar)
# BVHAR-----------
mean(mse_bvhar)
# Ghosh-----------
mean(mse_ghosh)
```



